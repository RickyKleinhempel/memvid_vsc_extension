{
  "name": "memvid-agent-memory",
  "displayName": "Memvid Agent Memory",
  "description": "Provides persistent AI Agent Memory using Memvid for GitHub Copilot agents. Automatically registers MCP tools for storing, searching, and retrieving agent memory.",
  "version": "0.1.4",
  "publisher": "RickyKleinhempel",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/RickyKleinhempel/memvid_vsc_extension"
  },
  "engines": {
    "vscode": "^1.102.0"
  },
  "categories": [
    "AI",
    "Other"
  ],
  "keywords": [
    "memvid",
    "agent",
    "memory",
    "copilot",
    "mcp",
    "ai",
    "rag",
    "vector-search"
  ],
  "icon": "images/icon.png",
  "activationEvents": [
    "onStartupFinished"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "mcpServerDefinitionProviders": [
      {
        "id": "memvid-agent-memory.mcp-server",
        "label": "Memvid Agent Memory"
      }
    ],
    "configuration": [
      {
        "title": "General",
        "properties": {
          "memvidAgentMemory.memoryFilePath": {
            "type": "string",
            "default": "",
            "description": "Custom path to the memory file (.mv2). If empty, uses workspace .memvid/agent-memory.mv2 or global storage."
          },
          "memvidAgentMemory.autoCreateMemory": {
            "type": "boolean",
            "default": true,
            "description": "Automatically create memory file if it doesn't exist."
          },
          "memvidAgentMemory.enableSemanticSearch": {
            "type": "boolean",
            "default": true,
            "description": "Enable semantic/vector search (requires embedding provider)."
          },
          "memvidAgentMemory.defaultSearchLimit": {
            "type": "number",
            "default": 10,
            "minimum": 1,
            "maximum": 100,
            "description": "Default number of search results to return."
          }
        }
      },
      {
        "title": "Embedding Provider",
        "properties": {
          "memvidAgentMemory.embeddingProvider": {
            "type": "string",
            "enum": [
              "none",
              "local",
              "openai",
              "azureOpenai",
              "ollama",
              "cohere",
              "voyage"
            ],
            "default": "none",
            "enumDescriptions": [
              "No embeddings - BM25 keyword search only",
              "Local embeddings (bge-small, nomic) - Not available on Windows",
              "OpenAI embeddings (text-embedding-3-small)",
              "Azure OpenAI embeddings",
              "Ollama local embeddings",
              "Cohere embeddings",
              "Voyage embeddings"
            ],
            "description": "Embedding provider for semantic search. Configure the selected provider below."
          }
        }
      },
      {
        "title": "OpenAI",
        "properties": {
          "memvidAgentMemory.openai.apiKey": {
            "type": "string",
            "default": "",
            "description": "OpenAI API Key. Can also be set via OPENAI_API_KEY environment variable."
          },
          "memvidAgentMemory.openai.baseUrl": {
            "type": "string",
            "default": "https://api.openai.com/v1",
            "description": "OpenAI API Base URL. Change for OpenAI-compatible endpoints."
          },
          "memvidAgentMemory.openai.model": {
            "type": "string",
            "default": "text-embedding-3-small",
            "description": "OpenAI embedding model to use."
          }
        }
      },
      {
        "title": "Azure OpenAI",
        "properties": {
          "memvidAgentMemory.azureOpenai.endpoint": {
            "type": "string",
            "default": "",
            "description": "Azure OpenAI endpoint URL (e.g., https://your-resource.openai.azure.com)"
          },
          "memvidAgentMemory.azureOpenai.apiKey": {
            "type": "string",
            "default": "",
            "description": "Azure OpenAI API Key. Can also be set via AZURE_OPENAI_API_KEY environment variable."
          },
          "memvidAgentMemory.azureOpenai.deploymentName": {
            "type": "string",
            "default": "",
            "description": "Azure OpenAI deployment name for embeddings."
          },
          "memvidAgentMemory.azureOpenai.apiVersion": {
            "type": "string",
            "default": "2024-02-01",
            "description": "Azure OpenAI API version."
          }
        }
      },
      {
        "title": "Ollama Embedding",
        "properties": {
          "memvidAgentMemory.ollama.baseUrl": {
            "type": "string",
            "default": "http://localhost:11434",
            "description": "Ollama server URL."
          },
          "memvidAgentMemory.ollama.model": {
            "type": "string",
            "default": "nomic-embed-text",
            "enum": [
              "nomic-embed-text",
              "mxbai-embed-large",
              "all-minilm",
              "snowflake-arctic-embed"
            ],
            "description": "Ollama embedding model to use."
          }
        }
      },
      {
        "title": "LLM Provider (Answer Generation)",
        "properties": {
          "memvidAgentMemory.llmProvider": {
            "type": "string",
            "enum": [
              "none",
              "copilot",
              "openai",
              "azureOpenai",
              "ollama"
            ],
            "default": "none",
            "enumDescriptions": [
              "No LLM - Returns context only without synthesis",
              "GitHub Copilot - Uses your existing Copilot subscription (recommended)",
              "OpenAI GPT models (gpt-4o, gpt-4o-mini, etc.)",
              "Azure OpenAI chat models",
              "Ollama local chat models (llama3, mistral, etc.)"
            ],
            "description": "LLM provider for generating answers from retrieved context. If 'none', memvid_ask returns context snippets only."
          }
        }
      },
      {
        "title": "LLM - GitHub Copilot",
        "properties": {
          "memvidAgentMemory.llmCopilot.modelFamily": {
            "type": "string",
            "default": "gpt-4o",
            "enum": [
              "gpt-4o",
              "gpt-4o-mini",
              "claude-3.5-sonnet",
              "o1",
              "o1-mini"
            ],
            "enumDescriptions": [
              "GPT-4o - Best balance of speed and quality",
              "GPT-4o Mini - Faster, more cost-effective",
              "Claude 3.5 Sonnet - Anthropic's latest model",
              "o1 - OpenAI reasoning model",
              "o1-mini - Smaller reasoning model"
            ],
            "description": "Preferred Copilot model family. Availability depends on your Copilot subscription."
          }
        }
      },
      {
        "title": "LLM - OpenAI",
        "properties": {
          "memvidAgentMemory.llmOpenai.apiKey": {
            "type": "string",
            "default": "",
            "description": "OpenAI API Key for LLM. Leave empty to use the embedding API key."
          },
          "memvidAgentMemory.llmOpenai.baseUrl": {
            "type": "string",
            "default": "https://api.openai.com/v1",
            "description": "OpenAI API Base URL for LLM. Change for OpenAI-compatible endpoints."
          },
          "memvidAgentMemory.llmOpenai.model": {
            "type": "string",
            "default": "gpt-4o-mini",
            "enum": [
              "gpt-4o",
              "gpt-4o-mini",
              "gpt-4-turbo",
              "gpt-3.5-turbo"
            ],
            "description": "OpenAI chat model to use for answer generation."
          },
          "memvidAgentMemory.llmOpenai.maxTokens": {
            "type": "number",
            "default": 1024,
            "minimum": 100,
            "maximum": 16384,
            "description": "Maximum tokens for LLM response."
          },
          "memvidAgentMemory.llmOpenai.temperature": {
            "type": "number",
            "default": 0.7,
            "minimum": 0,
            "maximum": 2,
            "description": "Temperature for LLM generation (0 = deterministic, higher = more creative)."
          }
        }
      },
      {
        "title": "LLM - Azure OpenAI",
        "properties": {
          "memvidAgentMemory.llmAzureOpenai.endpoint": {
            "type": "string",
            "default": "",
            "description": "Azure OpenAI endpoint URL for LLM. Leave empty to use the embedding endpoint."
          },
          "memvidAgentMemory.llmAzureOpenai.apiKey": {
            "type": "string",
            "default": "",
            "description": "Azure OpenAI API Key for LLM. Leave empty to use the embedding API key."
          },
          "memvidAgentMemory.llmAzureOpenai.deploymentName": {
            "type": "string",
            "default": "",
            "description": "Azure OpenAI deployment name for chat model (e.g., gpt-4o)."
          },
          "memvidAgentMemory.llmAzureOpenai.apiVersion": {
            "type": "string",
            "default": "2024-02-01",
            "description": "Azure OpenAI API version for LLM."
          },
          "memvidAgentMemory.llmAzureOpenai.maxTokens": {
            "type": "number",
            "default": 1024,
            "minimum": 100,
            "maximum": 16384,
            "description": "Maximum tokens for LLM response."
          },
          "memvidAgentMemory.llmAzureOpenai.temperature": {
            "type": "number",
            "default": 0.7,
            "minimum": 0,
            "maximum": 2,
            "description": "Temperature for LLM generation."
          }
        }
      },
      {
        "title": "LLM - Ollama",
        "properties": {
          "memvidAgentMemory.llmOllama.baseUrl": {
            "type": "string",
            "default": "http://localhost:11434",
            "description": "Ollama server URL for LLM. Leave empty to use the embedding server URL."
          },
          "memvidAgentMemory.llmOllama.model": {
            "type": "string",
            "default": "llama3.2",
            "enum": [
              "llama3.2",
              "llama3.1",
              "mistral",
              "mixtral",
              "phi3",
              "gemma2",
              "qwen2.5"
            ],
            "description": "Ollama chat model to use for answer generation."
          },
          "memvidAgentMemory.llmOllama.maxTokens": {
            "type": "number",
            "default": 1024,
            "minimum": 100,
            "maximum": 16384,
            "description": "Maximum tokens for LLM response."
          },
          "memvidAgentMemory.llmOllama.temperature": {
            "type": "number",
            "default": 0.7,
            "minimum": 0,
            "maximum": 2,
            "description": "Temperature for LLM generation."
          }
        }
      }
    ],
    "commands": [
      {
        "command": "memvidAgentMemory.initialize",
        "title": "Memvid: Initialize Memory"
      },
      {
        "command": "memvidAgentMemory.openMemory",
        "title": "Memvid: Open Agent Memory File"
      },
      {
        "command": "memvidAgentMemory.clearMemory",
        "title": "Memvid: Clear Agent Memory"
      },
      {
        "command": "memvidAgentMemory.showStats",
        "title": "Memvid: Show Memory Statistics"
      },
      {
        "command": "memvidAgentMemory.refreshMcp",
        "title": "Memvid: Refresh MCP Server"
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "npm run package-ext",
    "compile": "npm run check-types && node esbuild.js",
    "watch": "npm-run-all -p watch:*",
    "watch:esbuild": "node esbuild.js --watch",
    "watch:tsc": "tsc --noEmit --watch --project tsconfig.json",
    "check-types": "tsc --noEmit",
    "package-ext": "npm run check-types && node esbuild.js --production",
    "pretest": "npm run compile && npm run lint",
    "lint": "eslint src --ext ts",
    "test": "node ./out/test/runTest.js",
    "package": "vsce package",
    "publish": "vsce publish"
  },
  "bundledDependencies": [
    "@memvid/sdk",
    "@modelcontextprotocol/sdk"
  ],
  "devDependencies": {
    "@types/node": "^20.11.0",
    "@types/vscode": "^1.102.0",
    "@typescript-eslint/eslint-plugin": "^7.0.0",
    "@typescript-eslint/parser": "^7.0.0",
    "@vscode/test-electron": "^2.3.8",
    "@vscode/vsce": "^2.22.0",
    "esbuild": "^0.27.2",
    "eslint": "^8.56.0",
    "npm-run-all": "^4.1.5",
    "typescript": "^5.3.0"
  },
  "dependencies": {
    "@memvid/sdk": "^2.0.146",
    "@modelcontextprotocol/sdk": "^1.25.2"
  },
  "bundleDependencies": [
    "@memvid/sdk",
    "@modelcontextprotocol/sdk"
  ]
}
